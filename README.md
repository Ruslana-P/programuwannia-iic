# Machine Era Django Project

"Machine Era" — це навчальний та демонстраційний проект, який дозволяє користувачам завантажувати зображення, відео та аудіо для аналізу за допомогою сучасних AI-моделей. Проект показує інтеграцію штучного інтелекту у веб-додатки на базі Django."

Проект побудований на стандартній архітектурі Django: використовується `manage.py`, базові налаштування у `settings.py`, URL-маршрути у `urls.py`, база даних за замовчуванням SQLite. Додаток `machine_era` містить логіку для кожного модуля та шаблони для фронтенду.

---

## Системні вимоги

- Python >= 3.10  
- Django >= 4.2  
- TensorFlow >= 2.12  
- ultralytics >= 8.0  
- Pydub >= 0.25  
- opencv-python >= 4.7  

---

## Структура проекту

## Project Structure

```text
djangotutorial/
├── manage.py
├── db.sqlite3
└── mysite/
    ├── settings.py
    ├── urls.py
    └── ...

machine_era/
├── ai_model.py        # Image classification (VGG16)
├── ai_video_model.py  # Video object detection (YOLOv8)
├── ai_audio_model.py  # Audio analysis (YAMNet)
├── ai_spectrum_model.py  # Signal spectrum analysis (FFT)
├── ai_text_model.py      # Text readability analysis (Flesch Reading Ease)
├── views.py
├── forms.py
├── models.py
├── templates/
└── static/
```


### Головний дашборд

Головна сторінка проекту (`main_dashboard`) служить меню для модулів:  
1. **Image recognition (Module Alpha)**  
2. **Video element scan (Module Beta)**  
3. **Audio signature analysis (Module Gamma)**  
4. **Signal spectrum decode (Module Delta)**
5. **CUSTOM AI VARIANT (Module Epsilon)** – ще не реалізовано  

Кожен модуль має власну форму для завантаження файлів і сторінку з результатами аналізу.

### Module 1 - Image recognition
Цей модуль відповідає за завантаження зображень користувачем та їх класифікацію за допомогою штучного інтелекту.

В основі використана **VGG16**. VGG16 — це алгоритм розпізнавання та класифікації об'єктів, який може класифікувати 1000 категорій зображень з високою точністю. Модель популярна для задач класифікації зображень та легко використовується з трансферним навчанням (transfer learning).

 **Принцип роботи**
1. Зображення автоматично масштабуються до **224×224 пікселів**, щоб відповідати вимогам моделі.  
2. Зображення перетворюється у масив і проходить нормалізацію.  
3. Модель прогнозує ймовірності для всіх класів.  
4. Повертаються **топ-3 класи** з відсотком впевненості.

 **Підтримувані файли**
- **Формати:** JPEG, PNG, BMP, GIF  
- **Максимальна вага:** ~5 МБ

 **Note**
- Для коректної роботи потрібно **скачати ваги VGG16** (`vgg16_weights_tf_dim_ordering_tf_kernels.h5`) і помістити їх у папку `djangotutorial/`.  


### Module 2 - Video element scan 

Цей модуль відповідає за завантаження відео користувачем та визначення об’єктів на ключових кадрах за допомогою штучного інтелекту.

В основі використана **YOLOv8-nano**. Модель попередньо навчена на датасеті **COCO** і розпізнає **80 класів** (люди, автомобілі, тварини, предмети тощо). YOLOv8-nano оптимізована для швидкого аналізу відео з високою точністю на обмеженій кількості кадрів.  
**Файл моделі:** `yolov8n.pt` (повинен бути у проекті перед запуском).

**Принцип роботи**
1. Відео відкривається і обробляються ключові кадри (кожен 10-й кадр, до максимуму 30 кадрів).  
2. Кожен кадр аналізується моделлю YOLOv8 для виявлення об’єктів.  
3. Збираються всі виявлені класи об’єктів.  
4. Повертаються **топ-3 найпоширеніших об’єктів** та домінуючий об’єкт у відео.

**Підтримувані файли**
- **Формати:** MP4, MOV, WebM, OGG  
- **Максимальна вага:** залежить від налаштувань Django, рекомендовано ≤50 МБ  

**NOTE**
- Модель **YOLOv8-nano** автоматично завантажується при першому запуску модуля і зберігається в кеші для повторного використання.  
- Для швидкості та економії ресурсів відео аналізується лише на обмеженій кількості ключових кадрів.

### Module 3 - Audio signature analysis

Цей модуль відповідає за завантаження аудіо користувачем та розпізнавання звукових подій за допомогою штучного інтелекту.

**Модель**
В основі використана **YAMNet** від Google, завантажена через TensorFlow Hub. Модель попередньо навчена на датасеті **AudioSet** і розпізнає **521 клас звуків** (людські голоси, тварини, транспорт, інструменти та інші аудіоподії).  
Модель автоматично завантажується при першому запуску модуля і кешується для повторного використання.

**Принцип роботи**
1. Аудіофайл завантажується та приводиться до формату **моно, 16 кГц**, який вимагає YAMNet.  
2. З аудіо екстрагуються особливості (waveform).  
3. Модель прогнозує ймовірності всіх класів звуків.  
4. Визначається клас з найбільшою ймовірністю та повертається **найпоширеніша подія** з відсотком впевненості.

**Підтримувані файли**
- **Формати:** WAV, MP3, OGG та інші формати, розпізнавані бібліотекою Pydub  
- **Максимальна вага:** залежить від налаштувань Django, рекомендовано ≤20 МБ

**NOTE**
- Модель **YAMNet** автоматично завантажується з TensorFlow Hub при першому запуску і кешується для повторного використання.  

### Module 4 - Signal spectrum decode (Module Delta)

Цей модуль відповідає за завантаження сигналу користувачем (аудіофайлу) та аналіз його спектру за допомогою швидкого перетворення Фурʼє (FFT).

В основі використовується звʼязка **Pydub + NumPy**. Pydub через `ffmpeg` конвертує вхідний файл у моно‑сигнал 16 кГц, після чого NumPy обчислює амплітудний спектр. На основі спектру модуль виділяє домінуючі частоти та робить просту інтерпретацію типу сигналу (низькочастотний шум, мовний діапазон, високочастотний/шумовий сигнал).

**Принцип роботи**
1. Користувач завантажує аудіофайл через форму (модель `SignalUpload`).
2. Pydub зчитує файл і перетворює його в **mono, 16 kHz** з нормалізованою амплітудою.
3. До сигналу застосовується вікно Хеннінга і обчислюється **FFT** (`numpy.fft.rfft`).
4. За спектром обираються **топ‑K домінуючих частот** та їх відносна енергія.
5. Обчислюється середньозважена частота і на її основі визначається **тип спектру**:
    - LOW-FREQUENCY / HUM-LIKE SIGNAL
    - MID-BAND / SPEECH-LIKE SIGNAL
    - HIGH-FREQUENCY / NOISY SIGNAL
6. Сформований текстовий підсумок зберігається у полі `spectrum_summary` та відображається в історії аналізів.

**Підтримувані файли**
- **Формати:** WAV, MP3, OGG та інші, які підтримує Pydub/ffmpeg
- **Максимальна вага:** залежить від налаштувань Django, рекомендовано ≤ 20 МБ

**NOTE**
- Для коректної роботи модуля Delta обов’язково потрібен встановлений **ffmpeg/ffprobe** у системі.
- Якщо `ffmpeg` відсутній, модуль повертає контрольоване повідомлення `[SPECTRUM.ERROR]`, яке вказує, що спектральний аналіз неможливий у поточному середовищі.
- Результати аналізу зберігаються в моделі `SignalUpload` і доступні через окрему сторінку логів (історію запусків модуля).

### Module 5 - Text readability scan (Module Epsilon)

Цей модуль відповідає за завантаження текстового файлу користувачем та автоматичний аналіз **складності (читабельності)** тексту.

В основі використовується метрика **Flesch Reading Ease**. Це класичний показник, який оцінює, наскільки текст легкий чи важкий для сприйняття. На основі довжини речень та кількості складів модуль повертає числовий бал та текстовий вердикт (наприклад, *EASY*, *STANDARD*, *DIFFICULT*).

**Принцип роботи**
1. Користувач завантажує текстовий файл через форму (модель `TextUpload`).
2. Файл зчитується на сервері, вміст перетворюється у звичайний текст (`input_text`).
3. Функція аналізу `analyze_readability` рахує кількість слів, речень та складів.
4. Обчислюється **Flesch Reading Ease score** і формується вердикт (дуже легко, стандартно, важко тощо).
5. Результат (бал та вердикт) зберігається в базі даних і відображається в журналі аналізів.

**Підтримувані файли**
- **Формати:** `.txt`, `.md`, `.csv`, `.log` та інші текстові файли, що інтерпретуються як `text/plain`.
- **Мова тексту:** переважно англомовний текст (метрика розрахована під англійську).
- **Максимальний розмір:** залежить від налаштувань Django, рекомендовано ≤ 1–2 МБ.

**Note**
- Якщо файл неможливо коректно прочитати у кодуванні UTF‑8, неприпустимі символи ігноруються.
- Модуль не потребує додаткових ваг моделей чи зовнішніх файлів – уся логіка аналізу реалізована у власній функції `analyze_readability`.
--- 
## Installation

### Prerequisites
- Python 3.9+
- pip
- virtualenv (recommended)
- ffmpeg (required for audio processing with Pydub)

---

### Setup

1. **Clone the repository**
```bash
git clone <repository-url>
cd <project-root>
```

2. Create and activate a virtual environment
```bash
python3 -m venv tutorial-env
source tutorial-env/bin/activate
```

3.Install dependencies
```bash
pip install -r requirements.txt
```
4. Apply database migrations
```bash
cd djangotutorial
python manage.py migrate
```
5. Download required AI model files 
 - Download vgg16_weights_tf_dim_ordering_tf_kernels.h5

Place the file into:
```bash
djangotutorial/
```

Run the development server
```bash
cd ./djangotutorial
python manage.py runserver
```
Open the application
```bash
http://127.0.0.1:8000/machine_era/video-results/
```